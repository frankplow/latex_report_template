@misc{ApacheParquet,
  title = {Apache {{Parquet}}},
  urldate = {2024-01-19},
  abstract = {The Apache Parquet Website},
  howpublished = {https://parquet.apache.org/},
  langid = {english},
  file = {/Users/frank/Zotero/storage/NMU9X876/parquet.apache.org.html}
}

@misc{AttentionTransformersNeural,
  title = {"{{Attention}}", "{{Transformers}}", in {{Neural Network}} "{{Large Language Models}}"},
  urldate = {2023-12-25},
  howpublished = {http://bactra.org/notebooks/nn-attention-and-transformers.html\#transformers},
  file = {/Users/frank/Zotero/storage/589NS4YY/nn-attention-and-transformers.html}
}

@misc{barmanBjontegaardDeltaBD2024,
  title = {Bj{\o}ntegaard {{Delta}} ({{BD}}): {{A Tutorial Overview}} of the {{Metric}}, {{Evolution}}, {{Challenges}}, and {{Recommendations}}},
  shorttitle = {Bj\{{\textbackslash}o\}ntegaard {{Delta}} ({{BD}})},
  author = {Barman, Nabajeet and Martini, Maria G. and Reznik, Yuriy},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04039},
  eprint = {2401.04039},
  primaryclass = {cs, eess, math},
  publisher = {{arXiv}},
  urldate = {2024-01-15},
  abstract = {The Bj{\o}ntegaard Delta (BD) method proposed in 2001 has become a popular tool for comparing video codec compression efficiency. It was initially proposed to compute bitrate and quality differences between two Rate-Distortion curves using PSNR as distortion metric. Over the years, many works have calculated and reported BD results using other objective quality metrics such as SSIM, VMAF and, in some cases, even subjective ratings (mean opinion scores). However, the lack of consolidated literature explaining the metric, its evolution over the years, and a systematic evaluation of the same under different test conditions can result in a wrong interpretation of the BD results thus obtained.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Multimedia,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/frank/Zotero/storage/5T5CGWL8/Barman et al. - 2024 - Bj o ntegaard Delta (BD) A Tutorial Overview of .pdf}
}

@article{bossenVVCComplexitySoftware2021,
  title = {{{VVC Complexity}} and {{Software Implementation Analysis}}},
  author = {Bossen, Frank and Suhring, Karsten and Wieckowski, Adam and Liu, Shan},
  year = {2021},
  month = oct,
  journal = {IEEE Trans. Circuits Syst. Video Technol.},
  volume = {31},
  number = {10},
  pages = {3765--3778},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2021.3072204},
  urldate = {2023-11-19},
  abstract = {A steady increase in available processing power continues to drive advances in video compression technology. The recently completed Versatile Video Coding (VVC) standard aims to double the compression efficiency of HEVC and deliver a same quality of video at half the bitrate. To achieve this goal, VVC includes several new methods that improve coding efficiency at the cost of increased complexity. This paper provides a complexity analysis of VVC and its VTM reference software. Whereas VVC is more complex than HEVC, it remains readily implementable in software on current generation processors. Performance of practical decoders are reported, showing that real-time decoding of 8K content is feasible. An encoder is also presented, showing that most of the compression gains of VVC over HEVC can be obtained at a small fraction of the resources needed by the VTM encoder under common test conditions.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/BDYKKD52/Bossen et al. - 2021 - VVC Complexity and Software Implementation Analysi.pdf}
}

@article{brossOverviewVersatileVideo2021,
  title = {Overview of the {{Versatile Video Coding}} ({{VVC}}) {{Standard}} and Its {{Applications}}},
  author = {Bross, Benjamin and Wang, Ye-Kui and Ye, Yan and Liu, Shan and Chen, Jianle and Sullivan, Gary J. and Ohm, Jens-Rainer},
  year = {2021},
  month = oct,
  journal = {IEEE Trans. Circuits Syst. Video Technol.},
  volume = {31},
  number = {10},
  pages = {3736--3764},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2021.3101953},
  urldate = {2023-12-17},
  abstract = {Versatile Video Coding (VVC) was finalized in July 2020 as the most recent international video coding standard. It was developed by the Joint Video Experts Team (JVET) of the ITU-T Video Coding Experts Group (VCEG) and the ISO/IEC Moving Picture Experts Group (MPEG) to serve an ever-growing need for improved video compression as well as to support a wider variety of today's media content and emerging applications. This paper provides an overview of the novel technical features for new applications and the core compression technologies for achieving significant bit rate reductions in the neighborhood of 50\% over its predecessor for equal video quality, the High Efficiency Video Coding (HEVC) standard, and 75\% over the currently most-used format, the Advanced Video Coding (AVC) standard. It is explained how these new features in VVC provide greater versatility for applications. Highlighted applications include video with resolutions beyond standard- and high-definition, video with high dynamic range and wide color gamut, adaptive streaming with resolution changes, computergenerated and screen-captured video, ultralow-delay streaming, 360{$\smwhtcircle$} immersive video, and multilayer coding e.g., for scalability. Furthermore, early implementations are presented to show that the new VVC standard is implementable and ready for real-world deployment.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/UGMLF33Z/Bross et al. - 2021 - Overview of the Versatile Video Coding (VVC) Stand.pdf}
}

@book{bullIntelligentImageVideo2021,
  title = {Intelligent {{Image}} and {{Video Compression}}: {{Communicating Pictures}}},
  shorttitle = {Intelligent Image and Video Compression},
  author = {Bull, D. R. and Zhang, Fan},
  year = {2021},
  edition = {2},
  publisher = {{Elsevier}},
  address = {{London, United Kingdom; San Diego, CA, United States}},
  isbn = {978-0-12-820353-8},
  lccn = {TK6680.5 .B85 2021},
  keywords = {Coding theory,Compression vid{\'e}o,Digital video,Video compression,Vid{\'e}o num{\'e}rique},
  annotation = {OCLC: on1233315804},
  file = {/Users/frank/Zotero/storage/UVF9LYQP/Bull et al. - 2021 - Intelligent image and video compression communica.pdf}
}

@inproceedings{chachouEnergyConsumptionCarbon2023,
  title = {Energy {{Consumption}} and {{Carbon Footprint}} of {{Modern Video Decoding Software}}},
  booktitle = {2023 {{IEEE}} 25th {{International Workshop}} on {{Multimedia Signal Processing}} ({{MMSP}})},
  author = {Chachou, Taieb and Hamidouche, Wassim and Fezza, Sid Ahmed and Belalem, Ghalem},
  year = {2023},
  month = sep,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Poitiers, France}},
  doi = {10.1109/MMSP59012.2023.10337654},
  urldate = {2024-01-19},
  abstract = {The estimation of energy consumption has become vital in developing eco-friendly and sustainable video streaming solutions to monitor CO2 emissions. In this paper, we seek to evaluate and compare the energy consumption and CO2 emissions of the decoding process related to three popular video coding standards, namely AVC, HEVC, VVC, along with two video formats VP9, and AV1 through their real-time software decoders, including h264, hevc, VVdeC/OpenVVC, vp9, and libdav1d. The evaluation is conducted on two types of consumer hardware, desktop PC and laptop. To ensure a fair evaluation, we also assess the coding efficiency of software encoder implementations using three objective quality metrics. The experimental results revealed that the h264 decoder consumes the lowest energy and is associated with the lowest CO2 emissions compared to other decoders on both hardware platforms. On the other hand, the VVenC encoder enhances coding efficiency at the cost of increased decoding energy consumption and CO2 emissions, particularly noticeable in the case of the OpenVVC decoder. Meanwhile, x265/hevc achieves a compelling balance between coding efficiency and decoding energy consumption. The full results of this work are available at https://decodingenergy.github. io/decoding energy co2.html.},
  isbn = {979-8-350-33893-5},
  langid = {english},
  file = {/Users/frank/Zotero/storage/46PF3UF7/Chachou et al. - 2023 - Energy Consumption and Carbon Footprint of Modern .pdf}
}

@report{CiscoVisualNetworking2017,
  title = {Cisco {{Visual Networking Index}} 2017},
  year = {2017},
  institution = {{Cisco}},
  url = {https://www.cisco.com/c/dam/m/en_us/solutions/service-provider/vni-forecast-highlights/pdf/Global_Device_Growth_Traffic_Profiles.pdf},
  urldate = {2024-01-16},
  file = {/Users/frank/Zotero/storage/4RYCM3UN/Global_Device_Growth_Traffic_Profiles.pdf}
}

@misc{colemanSelectionProxyEfficient2020,
  title = {Selection via {{Proxy}}: {{Efficient Data Selection}} for {{Deep Learning}}},
  shorttitle = {Selection via {{Proxy}}},
  author = {Coleman, Cody and Yeh, Christopher and Mussmann, Stephen and Mirzasoleiman, Baharan and Bailis, Peter and Liang, Percy and Leskovec, Jure and Zaharia, Matei},
  year = {2020},
  month = oct,
  number = {arXiv:1906.11829},
  eprint = {1906.11829},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-12-14},
  abstract = {Data selection methods, such as active learning and core-set selection, are useful tools for machine learning on large datasets. However, they can be prohibitively expensive to apply in deep learning because they depend on feature representations that need to be learned. In this work, we show that we can greatly improve the computational efficiency by using a small proxy model to perform data selection (e.g., selecting data points to label for active learning). By removing hidden layers from the target model, using smaller architectures, and training for fewer epochs, we create proxies that are an order of magnitude faster to train. Although these small proxy models have higher error rates, we find that they empirically provide useful signals for data selection. We evaluate this ``selection via proxy'' (SVP) approach on several data selection tasks across five datasets: CIFAR10, CIFAR100, ImageNet, Amazon Review Polarity, and Amazon Review Full. For active learning, applying SVP can give an order of magnitude improvement in data selection runtime (i.e., the time it takes to repeatedly train and select points) without significantly increasing the final error (often within 0.1\%). For core-set selection on CIFAR10, proxies that are over 10{\texttimes} faster to train than their larger, more accurate targets can remove up to 50\% of the data without harming the final accuracy of the target, leading to a 1.6{\texttimes} end-to-end training time improvement.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/frank/Zotero/storage/TYXWRDQ4/Coleman et al. - 2020 - Selection via Proxy Efficient Data Selection for .pdf}
}

@book{coverElementsInformationTheory,
  title = {Elements of {{Information Theory}}},
  author = {Cover, Thomas M and Thomas, Joy A},
  langid = {english},
  file = {/Users/frank/Zotero/storage/84LZG892/Cover and Thomas - ELEMENTS OF INFORMATION THEORY.pdf}
}

@inproceedings{decockNonSenseArtificial2021,
  title = {The ({{Non}})Sense of {{Artificial Intelligense}} in {{Real-Time Video Encoding}}},
  booktitle = {{{IBC2021 Tech Papers}}: {{Advances}} in {{Video Coding}}},
  author = {DeCock, Jan},
  year = {2021},
  month = dec,
  address = {{Amsterdam, The Netherlands}},
  url = {https://www.ibc.org/download?ac=18627},
  langid = {english},
  file = {/Users/frank/Zotero/storage/TYG5CZ58/Cock - THE (NON)SENSE OF ARTIFICIAL INTELLIGENCE IN REAL-.pdf}
}

@misc{dingAdvancesVideoCompression2021,
  title = {Advances {{In Video Compression System Using Deep Neural Network}}: {{A Review And Case Studies}}},
  shorttitle = {Advances {{In Video Compression System Using Deep Neural Network}}},
  author = {Ding, Dandan and Ma, Zhan and Chen, Di and Chen, Qingshuang and Liu, Zoe and Zhu, Fengqing},
  year = {2021},
  month = jan,
  number = {arXiv:2101.06341},
  eprint = {2101.06341},
  primaryclass = {eess},
  publisher = {{arXiv}},
  urldate = {2024-01-18},
  abstract = {Significant advances in video compression system have been made in the past several decades to satisfy the nearly exponential growth of Internet-scale video traffic. From the application perspective, we have identified three major functional blocks including pre-processing, coding, and postprocessing, that have been continuously investigated to maximize the end-user quality of experience (QoE) under a limited bit rate budget. Recently, artificial intelligence (AI) powered techniques have shown great potential to further increase the efficiency of the aforementioned functional blocks, both individually and jointly. In this article, we review extensively recent technical advances in video compression system, with an emphasis on deep neural network (DNN)-based approaches; and then present three comprehensive case studies. On pre-processing, we show a switchable texture-based video coding example that leverages DNN-based scene understanding to extract semantic areas for the improvement of subsequent video coder. On coding, we present an end-to-end neural video coding framework that takes advantage of the stacked DNNs to efficiently and compactly code input raw videos via fully data-driven learning. On post-processing, we demonstrate two neural adaptive filters to respectively facilitate the in-loop and post filtering for the enhancement of compressed frames. Finally, a companion website hosting the contents developed in this work can be accessed publicly at https://purdueviper.github.io/dnn-coding/.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/frank/Zotero/storage/B9CX8E9F/Ding et al. - 2021 - Advances In Video Compression System Using Deep Ne.pdf}
}

@article{duanmuFastModePartition2016,
  title = {Fast {{Mode}} and {{Partition Decision Using Machine Learning}} for {{Intra-Frame Coding}} in {{HEVC Screen Content Coding Extension}}},
  author = {Duanmu, Fanyi and Ma, Zhan and Wang, Yao},
  year = {2016},
  month = dec,
  journal = {IEEE J. Emerg. Sel. Topics Circuits Syst.},
  volume = {6},
  number = {4},
  pages = {517--531},
  issn = {2156-3357, 2156-3365},
  doi = {10.1109/JETCAS.2016.2597698},
  urldate = {2023-12-17},
  abstract = {This paper presents a fast mode and partition decision framework for screen content coding (SCC) based on machine learning. Extensive statistical studies and complexity evaluations are conducted to explore the distribution of different coding modes and their complexities. The proposed encoder scheme is designed based on the results from these studies. Firstly, a CU is classified as either a natural image block (NIB) or a screen content block (SCB). An SCB only goes through SCC modes at the current CU level. An NIB is further classified as ``partitioned'' or ``non-partitioned''. The partitioned block will bypass current level intra modes. The non-partitioned block is classified as ``directional'' or ``non-directional'' and only goes through a subset of intra candidates. Decision tree classifiers are designed with chosen features that can distinguish different types of blocks. Furthermore, additional mode/partition checking is terminated once the current mode coding rate is lower than a statistics-based threshold. Compared with HEVC-SCC reference software, our proposed fast encoder can balance the encoding efficiency and complexity by adjusting decision confidence thresholds and rate thresholds. Under all-intra configurations, our ``rate-distortion preserving'' setting can achieve 40\% complexity reduction with only 1.46\% BD-loss. Our ``complexity-reduction boosting'' setting can achieve 52\% complexity reduction with 3.65\% BD-loss.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/9QK6TKCK/Duanmu et al. - 2016 - Fast Mode and Partition Decision Using Machine Lea.pdf}
}

@inproceedings{duarteFastIntraMode2023,
  title = {Fast {{Intra Mode Decision Using Machine Learning}} for the {{Versatile Video Coding Standard}}},
  booktitle = {2023 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Duarte, Adson and Zatt, Bruno and Correa, Guilherme and Palomino, Daniel},
  year = {2023},
  month = may,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Monterey, CA, USA}},
  doi = {10.1109/ISCAS46773.2023.10181769},
  urldate = {2023-12-17},
  abstract = {This paper presents a fast intra mode decision solution for the VVC standard using machine learning. The idea is to reorder the evaluation of modes performed by the Rate-Distortion Optimization (RDO) process according to the modes occurrence rate. Based on the new evaluation order, three Decision Tree models were trained to skip the modes less likely to be chosen. The results show that the proposed solution achieves time savings of up to 15.57\% with coding efficiency degradation of only 0.41\% on average. When compared with related works, the proposed solution shows competitive results.},
  isbn = {978-1-66545-109-3},
  langid = {english},
  file = {/Users/frank/Zotero/storage/UBXB4F5K/Duarte et al. - 2023 - Fast Intra Mode Decision Using Machine Learning fo.pdf}
}

@inproceedings{duarteMachineLearningBasedSolution2023,
  title = {A {{Machine Learning-Based Solution}} to {{Accelerate}} the {{Intra Mode Decision}} for the {{VVC Standard}}},
  booktitle = {Proceedings of the 29th {{Brazilian Symposium}} on {{Multimedia}} and the {{Web}}},
  author = {Duarte, Adson and Oliveira, Anna and Zatt, Bruno and Correa, Guilherme and Palomino, Daniel},
  year = {2023},
  month = oct,
  pages = {73--81},
  publisher = {{ACM}},
  address = {{Ribeir{\~a}o Preto Brazil}},
  doi = {10.1145/3617023.3617034},
  urldate = {2023-12-17},
  abstract = {The VVC video coding standard achieves high compression rates due to innovative tools that were introduced mainly in the intra prediction. However, the high computational effort associated with the intra mode decision poses a challenge for real-time video coding applications. In this paper, we propose a machine learning-based solution to accelerate the intra mode decision of VVC. The intra modes are organized in three classes (Planar/DC, Angular and MIP) and a Decision Tree model is developed to predict the class of modes more likely to be chosen, avoiding the evaluation of the classes of modes with less chance to be the optimal. As a result, the proposed solution can reduce the total encoding time in 15.67\% on average with only 0.80\% of coding efficiency loss. When compared with related works, our solution presents good results.},
  isbn = {979-8-400-70908-1},
  langid = {english},
  file = {/Users/frank/Zotero/storage/88TYL7YL/Duarte et al. - 2023 - A Machine Learning-Based Solution to Accelerate th.pdf}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  month = jul,
  journal = {Journal of machine learning research},
  volume = {12},
  number = {61},
  pages = {2121--2159},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/76JBRJYN/Duchi et al. - Adaptive Subgradient Methods for Online Learning a.pdf}
}

@report{efoui-hessClimateCrisisUnsustainable2019,
  title = {Climate {{Crisis}}: {{The Unsustainable Use}} of {{Online Video}}},
  author = {{Efoui-Hess}, Maxime},
  year = {2019},
  month = jul,
  institution = {{The Shift Project}},
  file = {/Users/frank/Zotero/storage/SIPT7PDN/2019-02.pdf}
}

@book{efronComputerAgeStatistical2016,
  title = {Computer {{Age Statistical Inference}}: {{Algorithms}}, {{Evidence}}, and {{Data Science}}},
  shorttitle = {Computer {{Age Statistical Inference}}},
  author = {Efron, Bradley and Hastie, Trevor},
  year = {2016},
  month = jul,
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9781316576533},
  urldate = {2024-01-03},
  isbn = {978-1-107-14989-2 978-1-316-57653-3},
  langid = {english},
  file = {/Users/frank/Zotero/storage/MV2LHG2K/Efron and Hastie - 2016 - Computer Age Statistical Inference Algorithms, Ev.pdf}
}

@misc{guoDeepCoreComprehensiveLibrary2022,
  title = {{{DeepCore}}: {{A Comprehensive Library}} for {{Coreset Selection}} in {{Deep Learning}}},
  shorttitle = {{{DeepCore}}},
  author = {Guo, Chengcheng and Zhao, Bo and Bai, Yanbing},
  year = {2022},
  month = jun,
  number = {arXiv:2204.08499},
  eprint = {2204.08499},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-14},
  abstract = {Coreset selection, which aims to select a subset of the most informative training samples, is a long-standing learning problem that can benefit many downstream tasks such as data-efficient learning, continual learning, neural architecture search, active learning, etc. However, many existing coreset selection methods are not designed for deep learning, which may have high complexity and poor generalization performance. In addition, the recently proposed methods are evaluated on models, datasets, and settings of different complexities. To advance the research of coreset selection in deep learning, we contribute a comprehensive code library3, namely DeepCore, and provide an empirical study on popular coreset selection methods on CIFAR10 and ImageNet datasets. Extensive experiments on CIFAR10 and ImageNet datasets verify that, although various methods have advantages in certain experiment settings, random selection is still a strong baseline.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/frank/Zotero/storage/2GN52QGM/Guo et al. - 2022 - DeepCore A Comprehensive Library for Coreset Sele.pdf}
}

@misc{hammingYouYourResearch,
  title = {You and {{Your Research}}},
  author = {Hamming, Richard},
  urldate = {2023-11-25},
  howpublished = {https://www.cs.virginia.edu/{\textasciitilde}robins/YouAndYourResearch.html},
  file = {/Users/frank/Zotero/storage/2HKQNJGX/YouAndYourResearch.html}
}

@book{harrellRegressionModelingStrategies2015,
  title = {Regression {{Modeling Strategies}}: {{With Applications}} to {{Linear Models}}, {{Logistic}} and {{Ordinal Regression}}, and {{Survival Analysis}}},
  shorttitle = {Regression {{Modeling Strategies}}},
  author = {Harrell ,, Frank E.},
  year = {2015},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-19425-7},
  urldate = {2023-12-05},
  isbn = {978-3-319-19424-0 978-3-319-19425-7},
  langid = {english},
  file = {/Users/frank/Zotero/storage/CLPGAP99/Harrell , - 2015 - Regression Modeling Strategies With Applications .pdf}
}

@misc{harrellStatisticalThinkingDamage2017,
  title = {Statistical {{Thinking}} - {{Damage Caused}} by {{Classification Accuracy}} and {{Other Discontinuous Improper Accuracy Scoring Rules}}},
  author = {Harrell, Frank},
  year = {2017},
  month = mar,
  urldate = {2023-12-05},
  abstract = {Estimating tendencies is usually a more appropriate goal than classification, and classification leads to the use of discontinuous accuracy scores which give rise to misleading results.},
  howpublished = {https://hbiostat.org/blog/post/class-damage/},
  langid = {english},
  file = {/Users/frank/Zotero/storage/6AJYP5FI/class-damage.html}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  publisher = {{Springer}},
  langid = {english},
  file = {/Users/frank/Zotero/storage/4CTBKEXK/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-23},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers{\textemdash}8{\texttimes} deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/frank/Zotero/storage/QMG9K8PD/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}

@misc{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  number = {arXiv:1503.02531},
  eprint = {1503.02531},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2024-01-18},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/frank/Zotero/storage/EWY9NZNB/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf}
}

@book{jamesIntroductionStatisticalLearning2023,
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and Taylor, Jonathon},
  year = {2023},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer}},
  address = {{New York, NY, USA}},
  url = {https://hastie.su.domains/ISLP/ISLP_website.pdf.download.html},
  isbn = {978-3-031-38746-3},
  langid = {english},
  file = {/Users/frank/Zotero/storage/2IVH4LVU/ISLP_website.pdf}
}

@misc{jentzenMathematicalIntroductionDeep2023,
  title = {Mathematical {{Introduction}} to {{Deep Learning}}: {{Methods}}, {{Implementations}}, and {{Theory}}},
  shorttitle = {Mathematical {{Introduction}} to {{Deep Learning}}},
  author = {Jentzen, Arnulf and Kuckuck, Benno and {von Wurstemberger}, Philippe},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20360},
  eprint = {2310.20360},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2024-01-16},
  abstract = {This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-\{{\textbackslash}L\}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/frank/Zotero/storage/CIDMD53D/Jentzen et al. - 2023 - Mathematical Introduction to Deep Learning Method.pdf}
}

@inproceedings{kalvaUsingMachineLearning2007,
  title = {Using Machine Learning for Fast Intra {{MB}} Coding in {{H}}.264},
  booktitle = {Electronic {{Imaging}} 2007},
  author = {Kalva, Hari and Christodoulou, Lakis},
  editor = {Chen, Chang Wen and Schonfeld, Dan and Luo, Jiebo},
  year = {2007},
  month = jan,
  volume = {65082},
  address = {{San Jose, CA, USA}},
  doi = {10.1117/12.706024},
  urldate = {2024-01-16},
  abstract = {H.264 is a highly efficient and complex video codec. The complexity of the codec makes it difficult to use all its features in resource constrained mobile devices. This paper presents a machine learning approach to reducing the complexity of Intra encoding in H.264. Determining the macro block coding mode requires substantial computational resources in H.264 video encoding. The goal of this work to reduce MB mode computation from a search operation, as is done in the encoders today, to a computation. We have developed a methodology based on machine learning that computes the MB coding mode instead of searching for the best match thus reducing the complexity of Intra 16x16 coding by 17 times and Intra 4x4 MB coding by 12.5 times. The proposed approach uses simple mean value metrics at the block level to characterize the coding complexity of a macro block. A generic J4.8 classifier is used to build the decision trees to quickly determine the mode. We present a methodology for Intra MB coding. The results show that intra MB mode can be determined with over 90\% accuracy. The proposed can also be used for determining MB prediction modes with an accuracy varying between 70\% and 80\%.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/ZA4VS8CP/Kalva and Christodoulou - 2007 - Using machine learning for fast intra MB coding in.pdf}
}

@misc{karpathyRecipeTrainingNeural2019,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  author = {Karpathy, Andrej},
  year = {2019},
  month = apr,
  journal = {Andrej Karpathy Blog},
  urldate = {2024-01-08},
  howpublished = {http://karpathy.github.io/2019/04/25/recipe/},
  file = {/Users/frank/Zotero/storage/8C3QV9CS/recipe.html}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-13},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/frank/Zotero/storage/RJIPMG8I/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf}
}

@article{kirchhofferOverviewNeuralNetwork2022,
  title = {Overview of the {{Neural Network Compression}} and {{Representation}} ({{NNR}}) {{Standard}}},
  author = {Kirchhoffer, Heiner and Haase, Paul and Samek, Wojciech and Muller, Karsten and {Rezazadegan-Tavakoli}, Hamed and Cricri, Francesco and Aksu, Emre B. and Hannuksela, Miska M. and Jiang, Wei and Wang, Wei and Liu, Shan and Jain, Swayambhoo and {Hamidi-Rad}, Shahab and Racape, Fabien and Bailer, Werner},
  year = {2022},
  month = may,
  journal = {IEEE Trans. Circuits Syst. Video Technol.},
  volume = {32},
  number = {5},
  pages = {3203--3216},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2021.3095970},
  urldate = {2023-12-17},
  abstract = {Neural Network Coding and Representation (NNR) is the first international standard for efficient compression of neural networks (NNs). The standard is designed as a toolbox of compression methods, which can be used to create coding pipelines. It can be either used as an independent coding framework (with its own bitstream format) or together with external neural network formats and frameworks. For providing the highest degree of flexibility, the network compression methods operate per parameter tensor in order to always ensure proper decoding, even if no structure information is provided. The NNR standard contains compression-efficient quantization and deep context-adaptive binary arithmetic coding (DeepCABAC) as core encoding and decoding technologies, as well as neural network parameter pre-processing methods like sparsification, pruning, low-rank decomposition, unification, local scaling and batch norm folding. NNR achieves a compression efficiency of more than 97\% for transparent coding cases, i.e. without degrading classification quality, such as top-1 or top-5 accuracies. This paper provides an overview of the technical features and characteristics of NNR.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/C6ZDWG8D/Kirchhoffer et al. - 2022 - Overview of the Neural Network Compression and Rep.pdf}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  month = dec,
  address = {{Lake Tahoe, CA, USA}},
  url = {https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  urldate = {2024-01-15},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\% and 18.9\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/FD8R95LG/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@article{kuangMachineLearningBasedFast2020,
  title = {Machine {{Learning-Based Fast Intra Mode Decision}} for {{HEVC Screen Content Coding}} via {{Decision Trees}}},
  author = {Kuang, Wei and Chan, Yui-Lam and Tsang, Sik-Ho and Siu, Wan-Chi},
  year = {2020},
  month = may,
  journal = {IEEE Trans. Circuits Syst. Video Technol.},
  volume = {30},
  number = {5},
  pages = {1481--1496},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2019.2903547},
  urldate = {2023-12-17},
  abstract = {The Screen Content Coding (SCC) extension of High Efficiency Video Coding (HEVC) improves coding gain for screen content videos by introducing two new coding modes, intra block copy (IBC) and palette (PLT) modes. However, the coding gain is achieved at the increased cost of computational complexity. In this paper, we propose a decision tree based framework for fast intra mode decision by investigating various features in training sets. To avoid the exhaustive mode searching process, a sequential arrangement of decision trees is proposed to check each mode separately by inserting a classifier before checking a mode. As compared with the previous approaches that both IBC and PLT modes are checked for screen content blocks (SCBs), the proposed coding framework is more flexible which facilitates either IBC or PLT mode to be checked for SCBs such that computational complexity is further reduced. To enhance the accuracy of decision trees, dynamic features are introduced which reveal the unique intermediate coding information of a coding unit (CU). Then, if all Fig 1. NIB and SCB in the first frame of ``WebBrowsing''.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/688YZPMH/Kuang et al. - 2020 - Machine Learning-Based Fast Intra Mode Decision fo.pdf}
}

@inproceedings{laudeDeepLearningbasedIntra2016,
  title = {Deep Learning-Based Intra Prediction Mode Decision for {{HEVC}}},
  booktitle = {2016 {{Picture Coding Symposium}} ({{PCS}})},
  author = {Laude, Thorsten and Ostermann, Jorn},
  year = {2016},
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Nuremberg, Germany}},
  doi = {10.1109/PCS.2016.7906399},
  urldate = {2023-11-19},
  abstract = {The High Efficiency Video Coding standard and its screen content coding extension provide superior coding efficiency compared to predecessor standards. However, this coding efficiency is achieved at the expense of very complex encoders. One major complexity driver is the comprehensive rate distortion (RD) optimization. In this paper, we present a deep learning-based encoder control which replaces the conventional RD optimization for the intra prediction mode with deep convolutional neural network (CNN) classifiers. Thereby, we save the RD optimization complexity. Our classifiers operate independently of any encoder decisions and reconstructed sample values. Thus, no additional systematic latency is introduced. Furthermore, the loss in coding efficiency is negligible with an average value of 0.52\% over HM-16.6+SCM-5.2.},
  isbn = {978-1-5090-5966-9},
  langid = {english},
  keywords = {Flag},
  file = {/Users/frank/Zotero/storage/YLSXSLVF/Laude and Ostermann - 2016 - Deep learning-based intra prediction mode decision.pdf}
}

@inproceedings{liuDARTSDifferentiableArchitecture2019,
  title = {{{DARTS}}: {{Differentiable Architecture Search}}},
  booktitle = {Seventh {{International Conference}} on {{Learning Representations}}},
  author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  year = {2019},
  address = {{New Orleans, LA, USA}},
  abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/UUJWD2TW/Liu et al. - 2019 - DARTS DIFFERENTIABLE ARCHITECTURE SEARCH.pdf}
}

@article{liuDeepLearningBasedVideo2021,
  title = {Deep {{Learning-Based Video Coding}}: {{A Review}} and {{A Case Study}}},
  shorttitle = {Deep {{Learning-Based Video Coding}}},
  author = {Liu, Dong and Li, Yue and Lin, Jianping and Li, Houqiang and Wu, Feng},
  year = {2021},
  month = jan,
  journal = {ACM Comput. Surv.},
  volume = {53},
  number = {1},
  eprint = {1904.12462},
  primaryclass = {cs, eess},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3368405},
  urldate = {2024-01-14},
  abstract = {The past decade has witnessed great success of deep learning technology in many disciplines, especially in computer vision and image processing. However, deep learning-based video coding remains in its infancy. This paper reviews the representative works about using deep learning for image/video coding, which has been an actively developing research area since the year of 2015. We divide the related works into two categories: new coding schemes that are built primarily upon deep networks (deep schemes), and deep network-based coding tools (deep tools) that shall be used within traditional coding schemes or together with traditional coding tools. For deep schemes, pixel probability modeling and auto-encoder are the two approaches, that can be viewed as predictive coding scheme and transform coding scheme, respectively. For deep tools, there have been several proposed techniques using deep learning to perform intra-picture prediction, inter-picture prediction, cross-channel prediction, probability distribution prediction, transform, post- or in-loop filtering, down- and up-sampling, as well as encoding optimizations. According to the newest reports, deep schemes have achieved comparable or even higher compression efficiency than the stateof-the-art traditional schemes, such as High Efficiency Video Coding (HEVC) based scheme, for image coding; deep tools have demonstrated the compression capability beyond HEVC for video coding. However, deep schemes have not yet reached the current height of HEVC for video coding, and deep tools remain largely unexplored at many aspects including the tradeoff between compression efficiency and encoding/decoding complexity, the optimization for perceptual naturalness or semantic quality, the speciality and universality, the federated design of multiple deep tools, and so on. In the hope of advocating the research of deep learning-based video coding, we present a case study of our developed prototype video codec, namely Deep Learning Video Coding (DLVC). DLVC features two deep tools that are both based on convolutional neural network (CNN), namely CNN-based in-loop filter (CNN-ILF) and CNN-based block adaptive resolution coding (CNN-BARC). Both tools help improve the compression efficiency by a significant margin. With the two deep tools as well as other non-deep coding tools, DLVC is able to achieve on average 39.6\% and 33.0\% bits saving than HEVC, under random-access and low-delay configurations, respectively. The source code of DLVC has been released for future researches.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Multimedia,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/frank/Zotero/storage/4EHLV8YL/Liu et al. - 2021 - Deep Learning-Based Video Coding A Review and A C.pdf}
}

@misc{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  number = {arXiv:1711.05101},
  eprint = {1711.05101},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  urldate = {2023-12-13},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/frank/Zotero/storage/RRF46T49/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf}
}

@article{maBVIDVCTrainingDatabase2022,
  title = {{{BVI-DVC}}: {{A Training Database}} for {{Deep Video Compression}}},
  shorttitle = {{{BVI-DVC}}},
  author = {Ma, Di and Zhang, Fan and Bull, David R.},
  year = {2022},
  journal = {IEEE Trans. Multimedia},
  volume = {24},
  pages = {3847--3858},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2021.3108943},
  urldate = {2023-11-19},
  abstract = {Deep learning methods are increasingly being applied in the optimisation of video compression algorithms and can achieve significantly enhanced coding gains, compared to conventional approaches. Such approaches often employ Convolutional Neural Networks (CNNs) which are trained on databases with relatively limited content coverage. In this paper, a new extensive and representative video database, BVI-DVC, is presented for training CNN-based video compression systems, with specific emphasis on machine learning tools that enhance conventional coding architectures, including spatial resolution and bit depth up-sampling, post-processing and in-loop filtering. BVIDVC contains 800 sequences at various spatial resolutions from 270p to 2160p and has been evaluated on ten existing network architectures for four different coding tools. Experimental results show that this database produces significant improvements in terms of coding gains over five existing (commonly used) image/video training databases under the same training and evaluation configurations. The overall additional coding improvements by using the proposed database for all tested coding modules and CNN architectures are up to 10.3\% based on the assessment of PSNR and 8.1\% based on VMAF.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/VXNX8IH5/Ma et al. - 2022 - BVI-DVC A Training Database for Deep Video Compre.pdf}
}

@article{maImageVideoCompression2020,
  title = {Image and {{Video Compression With Neural Networks}}: {{A Review}}},
  shorttitle = {Image and {{Video Compression With Neural Networks}}},
  author = {Ma, Siwei and Zhang, Xinfeng and Jia, Chuanmin and Zhao, Zhenghui and Wang, Shiqi and Wang, Shanshe},
  year = {2020},
  month = jun,
  journal = {IEEE Trans. Circuits Syst. Video Technol.},
  volume = {30},
  number = {6},
  pages = {1683--1698},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2019.2910119},
  urldate = {2023-12-17},
  abstract = {In recent years, the image and video coding technologies have advanced by leaps and bounds. However, due to the popularization of image and video acquisition devices, the growth rate of image and video data is far beyond the improvement of the compression ratio. In particular, it has been widely recognized that there are increasing challenges of pursuing further coding performance improvement within the traditional hybrid coding framework. Deep convolution neural network which makes the neural network resurge in recent years and has achieved great success in both artificial intelligent and signal processing fields, also provides a novel and promising solution for image and video compression. In this paper, we provide a systematic, comprehensive and up-to-date review of neural network-based image and video compression techniques. The evolution and development of neural network-based compression methodologies are introduced for images and video respectively. More specifically, the cutting-edge video coding techniques by leveraging deep learning and HEVC framework are presented and discussed, which promote the state-of-the-art video coding performance substantially. Moreover, the end-to-end image and video coding frameworks based on neural networks are also reviewed, revealing interesting explorations on next generation image and video coding frameworks/standards. The most significant research works on the image and video coding related topics using neural networks are highlighted, and future trends are also envisioned. In particular, the joint compression on semantic and visual information is tentatively explored to formulate high efficiency signal representation structure for both human vision and machine vision, which are the two dominant signal receptors in the age of artificial intelligence.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/5RYVLVC7/Ma et al. - 2020 - Image and Video Compression With Neural Networks .pdf}
}

@misc{markdonniganMarkDonniganTalks2023,
  title = {Mark {{Donnigan Talks Sustainability}} in {{Video Streaming}} at {{IBC}} 2023 - {{NETINT}}},
  author = {{Mark Donnigan}},
  year = {2023},
  month = nov,
  url = {https://www.youtube.com/watch?v=bHyp9VEYhJ4},
  urldate = {2024-01-16},
  abstract = {Learn about NETINT - https://netint.com Learn about Mark Donnigan - https://growthstage.marketing I spoke at IBC 2023 about how @NETINT-Technologies benefits video streaming platforms and hyperscale service providers to meet their ESG and net-zero commitments. Increasing demand for digital services, including video streaming, has raised concerns about data centers' environmental impact and energy consumption. Sustainability has become a critical issue in addressing these challenges. Sustainability refers to the ability to meet present needs without compromising future generations' ability to meet their own needs. The growth of digital services and data centers has increased greenhouse gas emissions, contributing to climate change. Video encoding is a crucial technology that enables video streaming by converting files into formats suitable for transmission over networks or storage on devices. However, it also contributes to sustainability challenges due to high energy consumption requirements. As such, businesses are increasingly looking to balance the growing demand for digital services with sustainability concerns while maintaining competitiveness. The COVID-19 pandemic has accelerated the shift towards digital services and increased the demand for video streaming. This trend highlights the urgency of addressing sustainability challenges in this sector. Companies like NETINT are developing high-performance video processing solutions that use ASICs explicitly designed for the video encoding and processing task, reducing energy consumption and contributing towards sustainability goals. Governments can play a significant role in promoting sustainability by implementing policies and regulations encouraging businesses' adoption of sustainable technologies and practices. Collaboration between businesses, governments, and civil society is also essential in addressing these challenges effectively. In conclusion, as our reliance on digital services like video streaming grows exponentially, we must address their environmental impact through sustainable practices such as efficient data center management techniques or new technologies like ASICs explicitly designed for tasks like video encoding while balancing economic competitiveness with environmental responsibility.}
}

@incollection{marksCarbonFootprintStreaming2022,
  title = {The {{Carbon Footprint}} of {{Streaming Media}}: {{Problems}}, {{Calculations}}, {{Solutions}}},
  shorttitle = {The {{Carbon Footprint}} of {{Streaming Media}}},
  booktitle = {Film and {{Television Production}} in the {{Age}} of {{Climate Crisis}}},
  author = {Marks, Laura U. and Przedpe{\l}ski, Radek},
  editor = {K{\"a}{\"a}p{\"a}, Pietari and Vaughan, Hunter},
  year = {2022},
  pages = {207--234},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-98120-4_10},
  urldate = {2024-01-11},
  isbn = {978-3-030-98119-8 978-3-030-98120-4},
  langid = {english},
  file = {/Users/frank/Zotero/storage/P4ZE6TUT/Marks and Przedpełski - 2022 - The Carbon Footprint of Streaming Media Problems,.pdf}
}

@techreport{moraCE3relatedDecodersideIntra2018,
  type = {Contribution},
  title = {{{CE3-related}}: {{Decoder-side Intra Mode Derivation}}},
  author = {Mora, Elie},
  year = {2018},
  month = oct,
  number = {JVET-L0164},
  address = {{Macao, China}},
  institution = {{WG 05 MPEG Joint Video Coding Team(s) with ITU-T SG 16}},
  urldate = {2023-12-17},
  abstract = {This contribution proposes a new intra coding mode that is named, in the rest of this document, DIMD (Decoder-side Intra Mode Derivation). In DIMD mode, the intra prediction mode is no longer searched for at the encoder but rather derived using previously encoded neighboring pixels through a gradient analysis. DIMD is signaled for intra coded blocks using a simple flag. At the decoder, if the DIMD flag is true, the intra prediction mode is derived in the reconstruction process using the same previously encoded neighboring pixels. If not, the intra prediction mode is parsed from the bitstream as in classical intra coding mode. DIMD brings 0.3\% gain on average (and up to 0.7\% on some sequences) over VTM-2.0.1 in an All Intra configuration, with limited additional decoding complexity (2\%).},
  langid = {english},
  file = {/Users/frank/Zotero/storage/26L72GQ5/JVET-L0164-v2.docx}
}

@inproceedings{nasrallahDecoderSideIntraMode2019,
  title = {Decoder-{{Side Intra Mode Derivation}} with {{Texture Analysis}} in {{VVC Test Model}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Nasrallah, Anthony and Abdoli, Mohsen and Mora, Elie Gabriel and Guionnet, Thomas and Raulet, Mickael},
  year = {2019},
  month = sep,
  pages = {3153--3157},
  publisher = {{IEEE}},
  address = {{Taipei, Taiwan}},
  doi = {10.1109/ICIP.2019.8803773},
  urldate = {2023-12-17},
  abstract = {The standardization of the next generation video codec, called Versatile Video Coding (VVC), has been progressing fast since 2018. At the same video quality, the VVC Test Model (VTM) currently provides about 27\% bitrate saving compared to the preceding High Efficiency Video Coding (HEVC) standard. The ultimate objective is to reach about 50\% improvement by the end of 2020. This paper presents an intra coding algorithm for VTM that skips the mode signaling at the encoder side and leaves it to be derived at the decoder side. To this end, a Histogram of Gradient (HoG) is computed from the texture of the previously reconstructed pixels and processed to derive the intra mode. Moreover, the proposed method is tuned to minimize its side-effects on the existing intra coding tools. Experimental results, on top of VTM-3.0, show that the proposed algorithm brings -0.22\% BD-rate gain, with a negligible complexity.},
  isbn = {978-1-5386-6249-6},
  langid = {english},
  keywords = {Texture Analysis,VVC},
  file = {/Users/frank/Zotero/storage/8JDZZNJM/Nasrallah et al. - 2019 - Decoder-Side Intra Mode Derivation with Texture An.pdf}
}

@misc{nielsenPrinciplesEffectiveResearch2004,
  title = {Principles of {{Effective Research}}},
  author = {Nielsen, Michael},
  year = {2004},
  month = jul,
  urldate = {2023-11-25},
  langid = {american},
  file = {/Users/frank/Zotero/storage/WRZDHCDI/principles-of-effective-research.html}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer Series in Operation Research and Financial Engineering},
  edition = {Second edition},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-0-387-30303-1 978-1-4939-3711-0},
  langid = {english},
  file = {/Users/frank/Zotero/storage/MCRKUYB5/Nocedal and Wright - 2006 - Numerical optimization.pdf}
}

@article{paulDeepLearningData,
  title = {Deep {{Learning}} on a {{Data Diet}}: {{Finding Important Examples Early}} in {{Training}}},
  author = {Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
  abstract = {Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores{\textemdash}the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores{\textemdash}and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generalize to other configurations. Compared to recent work that prunes data by discarding examples that are rarely forgotten over the course of training, our scores use only local information early in training. We also use our scores to detect noisy examples and study training dynamics through the lens of important examples{\textemdash}we investigate how the data distribution shapes the loss surface and identify subspaces of the model's data representation that are relatively stable over training.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/3FJBMVH8/Paul et al. - Deep Learning on a Data Diet Finding Important Ex.pdf}
}

@article{pfaffIntraPredictionMode2021,
  title = {Intra {{Prediction}} and {{Mode Coding}} in {{VVC}}},
  author = {Pfaff, Jonathan and Filippov, Alexey and Liu, Shan and Zhao, Xin and Chen, Jianle and {De-Luxan-Hernandez}, Santiago and Wiegand, Thomas and Rufitskiy, Vasily and Ramasubramonian, Adarsh Krishnan and Van Der Auwera, Geert},
  year = {2021},
  month = oct,
  journal = {IEEE Trans. Circuits Syst. Video Technol.},
  volume = {31},
  number = {10},
  pages = {3834--3847},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2021.3072430},
  urldate = {2023-12-17},
  abstract = {This paper presents the intra prediction and mode coding of the Versatile Video Coding (VVC) standard. This standard was collaboratively developed by the Joint Video Experts Team (JVET). It follows the traditional architecture of a hybrid block-based codec that was also the basis of previous standards. Almost all intra prediction features of VVC either contain substantial modifications in comparison with its predecessor H.265/HEVC or were newly added. The key aspects of these tools are the following: 65 angular intra prediction modes with block shape-adaptive directions and 4-tap interpolation filters are supported as well as the DC and Planar mode, Position Dependent Prediction Combination is applied for most of these modes, Multiple Reference Line Prediction can be used, an intra block can be further subdivided by the Intra Subpartition mode, Matrix-based Intra Prediction is supported, and the chroma prediction signal can be generated by the Cross Component Linear Model method. Finally, the intra prediction mode in VVC is coded separately for luma and chroma. Here, a Most Probable Mode list containing six modes is applied for luma. The individual compression performance of tools is reported in this paper. For the full VVC intra codec, a bitrate saving of 25\% on average is reported over H.265/HEVC using an objective metric. Significant subjective benefits are illustrated with specific examples.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/LK98A58D/Pfaff et al. - 2021 - Intra Prediction and Mode Coding in VVC.pdf}
}

@misc{PythonApacheArrow,
  title = {Python {\textemdash} {{Apache Arrow}} V14.0.2},
  urldate = {2024-01-19},
  howpublished = {https://arrow.apache.org/docs/python/index.html},
  file = {/Users/frank/Zotero/storage/DM5U6WKD/index.html}
}

@standard{ReferenceSoftwareITUT2022,
  type = {Recommendation},
  title = {Reference Software for {{ITU-T H}}.266 Versatile Video Coding},
  year = {2022},
  month = apr,
  number = {ITU-T Recommendation H.266.2 (04/22)},
  publisher = {{International Telecommunications Union}},
  address = {{Geneva, Switzerland}},
  institution = {{Joint Video Experts Team}},
  url = {https://www.itu.int/rec/T-REC-H.266.2-202204-I/en},
  urldate = {2024-01-19},
  file = {/Users/frank/Zotero/storage/C2AFD2PF/en.html}
}

@book{richardson264AdvancedVideo2010,
  title = {The {{H}}.264 {{Advanced Video Compression Standard}}},
  author = {Richardson, Iain E. G.},
  year = {2010},
  edition = {2},
  publisher = {{Wiley}},
  address = {{Chichester, UK}},
  isbn = {978-0-470-51692-8},
  langid = {english},
  file = {/Users/frank/Zotero/storage/5BA7MDVG/Richardson - The H.264 Advanced Video Compression Standard, Sec.pdf;/Users/frank/Zotero/storage/94SFRFE8/Richardson - 2010 - The H.264 advanced video compression standard.pdf}
}

@report{SandvineGlobalInternet2023,
  title = {Sandvine {{Global Internet Phenomena Report}} 2023},
  year = {2023},
  month = jan,
  institution = {{Sandvine}},
  url = {https://www.sandvine.com/phenomena},
  langid = {english},
  file = {/Users/frank/Zotero/storage/TMN5VERD/Cantor - A word from Sandvine CEO Lyn Cantor.pdf}
}

@misc{senerActiveLearningConvolutional2018,
  title = {Active {{Learning}} for {{Convolutional Neural Networks}}: {{A Core-Set Approach}}},
  shorttitle = {Active {{Learning}} for {{Convolutional Neural Networks}}},
  author = {Sener, Ozan and Savarese, Silvio},
  year = {2018},
  month = jun,
  number = {arXiv:1708.00489},
  eprint = {1708.00489},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-12-15},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/frank/Zotero/storage/ER6SREMZ/Sener and Savarese - 2018 - Active Learning for Convolutional Neural Networks.pdf}
}

@book{serghiouOpenRigorousReproducible2021,
  title = {Open, Rigorous and Reproducible Research: {{A}} Practitioner's Handbook},
  shorttitle = {Open, Rigorous and Reproducible Research},
  author = {Serghiou, Yan Min, Stylianos, Dallas Card},
  year = {2021},
  month = dec,
  urldate = {2023-11-19},
  abstract = {Even though many scientists are expected to abide by the principles of open, rigorous and reproducible research, few scientists actually receive formal training in this. Indeed, there is no collection written by workings scientis for working scientists about the principles of open, rigorous and transparent research and how to go about practicing such research, beyond theory. This handbook represents a constantly updated guide aiming to fulfil this gap and by doing so actively assist all researchers in producing informative research.},
  file = {/Users/frank/Zotero/storage/UE8UQ3TS/index.html}
}

@misc{smithAIPoisedTransform2023,
  title = {{{AI Poised}} to {{Transform Video-Compression Landscape}} - {{IEEE Spectrum}}},
  author = {Smith, Craig S.},
  year = {2023},
  month = apr,
  journal = {IEEE Spectrum},
  urldate = {2024-01-18},
  abstract = {Apple's WaveOne purchase heralds a new era in smart-streaming of AR and video},
  howpublished = {https://spectrum.ieee.org/ai-video-codecs-waveone},
  langid = {english},
  file = {/Users/frank/Zotero/storage/PHWJNPM8/ai-video-codecs-waveone.html}
}

@misc{sobelHistoryDefinitionSocalled2014,
  title = {History and {{Definition}} of the So-Called "{{Sobel Operator}}", More Appropriately Named the {{Sobel-Feldman Operator}}},
  author = {Sobel, Irwin},
  year = {2014},
  month = feb,
  file = {/Users/frank/Zotero/storage/5JVLE8YC/sobel.txt}
}

@misc{StatisticalThinkingClassification,
  title = {Statistical {{Thinking}} - {{Classification}} vs.~{{Prediction}}},
  urldate = {2023-12-05},
  howpublished = {https://hbiostat.org/blog/post/classification/index.html},
  file = {/Users/frank/Zotero/storage/KHLGBRWL/index.html}
}

@report{sullivanMeetingReport13th2013,
  type = {Report {{Document}} from {{Chairs}} of {{JCT-VC}}},
  title = {Meeting Report of the 13th Meeting of the {{Joint Collaborative Team}} on {{Video Coding}} ({{JCT-VC}}),},
  author = {Sullivan, Gary J. and Ohm, Jens-Rainer},
  year = {2013},
  month = apr,
  number = {JCTVC-M1000},
  address = {{Incheon, KR}},
  institution = {{Joint Collaborative Team on Video Coding (JCT-VC) of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11}},
  file = {/Users/frank/Zotero/storage/3NCJYFPU/JCTVC-M1000.doc}
}

@book{szeHighEfficiencyVideo2014,
  title = {High {{Efficiency Video Coding}} ({{HEVC}}): {{Algorithms}} and {{Architectures}}},
  shorttitle = {High {{Efficiency Video Coding}} ({{HEVC}})},
  editor = {Sze, Vivienne and Budagavi, Madhukar and Sullivan, Gary J.},
  year = {2014},
  series = {Integrated {{Circuits}} and {{Systems}}},
  publisher = {{Springer}},
  address = {{Cham, Switzerland}},
  doi = {10.1007/978-3-319-06895-4},
  urldate = {2024-01-18},
  isbn = {978-3-319-06895-4},
  langid = {english},
  file = {/Users/frank/Zotero/storage/XLFMLABD/Sze et al. - 2014 - High Efficiency Video Coding (HEVC) Algorithms an.pdf}
}

@inproceedings{tissierComplexityReductionOpportunities2019,
  title = {Complexity {{Reduction Opportunities}} in the {{Future VVC Intra Encoder}}},
  booktitle = {2019 {{IEEE}} 21st {{International Workshop}} on {{Multimedia Signal Processing}} ({{MMSP}})},
  author = {Tissier, A. and Mercat, A. and Amestoy, T. and Hamidouche, W. and Vanne, J. and Menard, D.},
  year = {2019},
  month = sep,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Kuala Lumpur, Malaysia}},
  doi = {10.1109/MMSP.2019.8901754},
  urldate = {2024-01-11},
  abstract = {The Joint Video Expert Team (JVET) is developing the next-generation video coding standard called Versatile Video Coding (VVC) and their ultimate goal is to double the coding efficiency over the current state-of-the-art standard HEVC without letting complexity get out of hand. This work addresses the complexity of the VVC reference encoder called VVC Test Model (VTM) under All Intra coding configuration. The VTM3.0 is able to improve intra coding efficiency by 21\% over the latest HEVC reference encoder HM16.19. This coding gain primarily stems from three new coding tools. First, the HEVC QuadTree (QT) structure extension with Multi-Type Tree (MTT) partitioning. Second, the duplication of intra prediction modes from 35 to 67. And third, the Multiple Transform Selection (MTS) scheme with two new discrete cosine/sine transforms (DCT-VIII and DST-VII). However, these new tools also play an integral part in making VTM intra encoding around 20 times as complex as that of HM. The purpose of this work is to analyze these tools individually and specify theoretical upper limits for their complexity reduction. According to our evaluations, the complexity reduction opportunity of block partitioning is up to 97\%, i.e., the encoding complexity would drop down to 3\% for the same coding efficiency if the optimal block partitioning could be directly predicted. The respective percentages for intra mode reduction and MTS optimization are 65\% and 55\%. We believe these results motivate VVC codec designers to develop techniques that are able to take most out of these opportunities.},
  isbn = {978-1-72811-817-8},
  langid = {english},
  file = {/Users/frank/Zotero/storage/ECVDZXVX/Tissier et al. - 2019 - Complexity Reduction Opportunities in the Future V.pdf}
}

@misc{tonevaEmpiricalStudyExample2019,
  title = {An {{Empirical Study}} of {{Example Forgetting}} during {{Deep Neural Network Learning}}},
  author = {Toneva, Mariya and Sordoni, Alessandro and des Combes, Remi Tachet and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J.},
  year = {2019},
  month = nov,
  number = {arXiv:1812.05159},
  eprint = {1812.05159},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-12-14},
  abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a ``forgetting event'' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/frank/Zotero/storage/B4JPDKMG/Toneva et al. - 2019 - An Empirical Study of Example Forgetting during De.pdf}
}

@standard{VersatileVideoCoding2023,
  type = {Recommendation},
  title = {Versatile {{Video Coding}}},
  year = {2023},
  month = sep,
  number = {ITU-T Recommendation H.266 (09/23)},
  publisher = {{International Telecommunications Union}},
  address = {{Geneva, Switzerland}},
  institution = {{Joint Video Experts Team}},
  url = {https://www.itu.int/rec/T-REC-H.266-202309-I/en},
  abstract = {Recommendation ITU-T H.266 specifies a video coding technology known as Versatile Video Coding and it has been designed with two primary goals. The first of these is to specify a video coding technology with a compression capability that is substantially beyond that of the prior generations of such standards, and the second is for this technology to be highly versatile for effective use in a broadened range of applications than that addressed by prior standards. Some key application areas for the use of this standard particularly include ultra-high-definition video (e.g., with 3840{\texttimes}2160 or 7620{\texttimes}4320 picture resolution and bit depth of 10 bits as specified in Rec. ITU-R BT.2100), video with a high dynamic range and wide colour gamut (e.g., with the perceptual quantization or hybrid log-gamma transfer characteristics specified in Rec. ITU-R BT.2100), and video for immersive media applications such as 360{\textdegree} omnidirectional video projected using a common projection format such as the equirectangular or cubemap projection formats, in addition to the applications that have commonly been addressed by prior video coding standards. The third edition adds the specification of a new level (level 15.5) for the video profiles to provide a suitable label for bitstreams that can exceed the limits of all other specified levels, additional supplement enhancement information, and corrections to various minor defects in the prior content of the Recommendation. This Recommendation was developed collaboratively with ISO/IEC JTC 1/SC 29, and corresponds with ISO/IEC 23090-3 as technically aligned twin text.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/HNGJWLS6/T-REC-H.266-202309-I!!PDF-E.pdf}
}

@inproceedings{wilsonMarginalValueAdaptive2017,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  booktitle = {31st {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  year = {2017},
  address = {{Long Beach, CA, USA}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several stateof-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/WS4BT99F/Wilson et al. - The Marginal Value of Adaptive Gradient Methods in.pdf}
}

@inproceedings{yingminFastIntraMode2022,
  title = {Fast {{Intra Mode Decision Algorithm}} of {{HEVC Based}} on {{Convolutional Neural Network}}},
  booktitle = {2022 2nd {{Asia-Pacific Conference}} on {{Communications Technology}} and {{Computer Science}} ({{ACCTCS}})},
  author = {Yingmin, Yi and Zhaoyang, Zheng and Yiwei, Yuan and Xianghong, Xue and Yuxing, Li},
  year = {2022},
  month = feb,
  pages = {76--79},
  publisher = {{IEEE}},
  address = {{Shenyang, China}},
  doi = {10.1109/ACCTCS53867.2022.00023},
  urldate = {2023-11-19},
  abstract = {Aiming at the high complexity and large amount of computation of HEVC intra mode decision algorithm, a fast intra mode decision algorithm based on convolutional neural network is proposed. Firstly, different size of prediction units are scaled to the same size through bilinear interpolation, and then sent to convolution neural network to perform convolution pooling and other operations. Finally, the mode decision results are selected by softmax layer. Five standard test sequences with different resolutions are used to form data sets, and the convolutional neural network model is trained and tested. Experimental results show that the proposed algorithm is equivalent to the standard test software HM16.0 in image quality and video bit rate, but the total coding time is improved by about 20.14\%.},
  isbn = {978-1-66540-034-3},
  langid = {english},
  keywords = {Flag},
  file = {/Users/frank/Zotero/storage/RQEVHFKH/Yingmin et al. - 2022 - Fast Intra Mode Decision Algorithm of HEVC Based o.pdf}
}

@misc{zhaoAOMCommonTest2021,
  title = {{{AOM Common Test Conditions}} v2.0},
  author = {Zhao, Xin and Lei, Zhijun (Ryan) and Norkin, Andrey and Daede, Thomas and Tourapis, Alexis},
  year = {2021},
  month = aug,
  publisher = {{Alliance for Open Media, Codec Working Group}},
  url = {https://aomedia.org/docs/CWG-B075o_AV2_CTC_v2.pdf},
  urldate = {2024-01-19},
  langid = {english},
  organization = {{Alliance for Open Media, Codec Working Group}},
  file = {/Users/frank/Zotero/storage/973SYCC5/CWG-B075o_AV2_CTC_v2.pdf}
}

@article{zhaoTransformCodingVVC2021,
  title = {Transform {{Coding}} in the {{VVC Standard}}},
  author = {Zhao, Xin and Kim, Seung-Hwan and Zhao, Yin and Egilmez, Hilmi E. and Koo, Moonmo and Liu, Shan and Lainema, Jani and Karczewicz, Marta},
  year = {2021},
  month = oct,
  journal = {IEEE Trans. Circuits Syst. Video Technol.},
  volume = {31},
  number = {10},
  pages = {3878--3890},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2021.3087706},
  urldate = {2023-12-17},
  abstract = {In the past decade, the development of transform coding techniques has achieved significant progress and several advanced transform tools have been adopted in the new generation Versatile Video Coding (VVC) standard. In this paper, a brief history of transform coding development during VVC standardization is presented, and the transform coding tools in the VVC standard are described in detail together with their initial design, incremental improvements and implementation aspects. To improve coding efficiency, four new transform coding techniques are introduced in VVC, which are namely Multiple Transform Selection (MTS), Low-Frequency Non-separable Secondary Transform (LFNST) and Sub-Block Transform (SBT), as well as a large (64-point) type-2 DCT. The experimental results on VVC reference software (VTM-9.0) show that average 4.5\% and 3.6\% overall coding gain can be achieved by the VVC transform coding tools for All Intra and Random Access configurations, respectively.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/E4YRCDK5/Zhao et al. - 2021 - Transform Coding in the VVC Standard.pdf}
}

@article{zhuDeepLearningBasedIntra2023,
  title = {Deep {{Learning-Based Intra Mode Derivation}} for {{Versatile Video Coding}}},
  author = {Zhu, Linwei and Zhang, Yun and Li, Na and Jiang, Gangyi and Kwong, Sam},
  year = {2023},
  month = jun,
  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  volume = {19},
  number = {2s},
  pages = {1--20},
  issn = {1551-6857, 1551-6865},
  doi = {10.1145/3563699},
  urldate = {2023-11-19},
  abstract = {In intra coding,               Rate Distortion Optimization (RDO)               is performed to achieve the optimal intra mode from a pre-defined candidate list. The optimal intra mode is also required to be encoded and transmitted to the decoder side besides the residual signal, where lots of coding bits are consumed. To further improve the performance of intra coding in               Versatile Video Coding (VVC)               , an intelligent intra mode derivation method is proposed in this paper, termed as               Deep Learning based Intra Mode Derivation (DLIMD)               . In specific, the process of intra mode derivation is formulated as a multi-class classification task, which aims to skip the module of intra mode signaling for coding bits reduction. The architecture of DLIMD is developed to adapt to different quantization parameter settings and variable coding blocks including non-square ones, where only one single trained model is required. Different from the existing deep learning based classification problems, the hand-crafted features are also fed into intra mode derivation network besides the learned features from feature learning network. To compete with traditional methods, one additional binary flag is utilized in the video codec to indicate the selected scheme with RDO. Extensive experimental results reveal that the proposed method can achieve 2.28\%, 1.74\%, and 2.18\% bit rate reduction on average for Y, U, and V components on the platform of VVC test model, which outperforms the state-of-the-art works.},
  langid = {english},
  file = {/Users/frank/Zotero/storage/W79CVW9G/Zhu et al. - 2023 - Deep Learning-Based Intra Mode Derivation for Vers.pdf}
}
